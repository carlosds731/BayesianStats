{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hypothesis testing (continued)\n",
    "\n",
    "Let's calculate the probabilities of the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33205759 0.36551343 0.30242898]\n",
      "[0.1171875  0.21499085 0.26682793]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Different values of p\n",
    "p = [0.5, 0.6, 0.7]\n",
    "\n",
    "# A priori probabilities for each p\n",
    "probabilities_a_priori = [0.5, 0.3, 0.2]\n",
    "\n",
    "# Calculate the likelihoods\n",
    "binom_distribution = stats.binom(n=10, p=p)\n",
    "likelihoods = binom_distribution.pmf(7)\n",
    "\n",
    "# Calculate the probability of the intersections\n",
    "probability_intersection = probabilities_a_priori*likelihoods\n",
    "\n",
    "# Total probability of the data\n",
    "total_probability_of_data = sum(probability_intersection)\n",
    "\n",
    "# Calculate the posterior probabilities of each hypothesis\n",
    "posterior_probabilities = probability_intersection/total_probability_of_data\n",
    "\n",
    "print(posterior_probabilities)\n",
    "print(likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The posterior probability of H0 is 0.33\n",
      "The posterior probability of H1 is 0.67\n"
     ]
    }
   ],
   "source": [
    "posterior_probability_H_0=posterior_probabilities[0]\n",
    "posterior_probability_H_1=posterior_probabilities[1]+posterior_probabilities[2]\n",
    "print('The posterior probability of H0 is {:.2f}'.format(posterior_probability_H_0))\n",
    "print('The posterior probability of H1 is {:.2f}'.format(posterior_probability_H_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "This means that, \n",
    "\n",
    "$$P\\left( {{H_0}\\left| y \\right.} \\right) = 0.33$$\n",
    "$$P\\left( {{H_1}\\left| y \\right.} \\right) = 0.37 + 0.30 = 0.67.$$\n",
    "\n",
    "This shows that, given the data, $H_1$ is twice more likelly than $H_0$.\n",
    "\n",
    "In general, calculating the posterior probabilities of each hypothesis is not easy, that's why is easier to analyze the Bayes factor (although in this problem is not necessary because we already have the posterior probability for each hypothesis).\n",
    "\n",
    "As an example, let's calculate the Bayes factor for this problem.\n",
    "\n",
    "$$BF = \\frac{{P\\left( {y\\left| {{H_0}} \\right.} \\right)}}{{P\\left( {y\\left| {{H_1}} \\right.} \\right)}}.$$\n",
    "\n",
    "The value of ${P\\left( {y\\left| {{H_0}} \\right.} \\right)}$ we already have it, is just the probability of getting 7 heads in 10 tossings with a fair coin, \n",
    "\n",
    "$$P\\left( {y\\left| {{H_0}} \\right.} \\right) = \\left( \\begin{gathered}\n",
    "  10 \\hfill \\\\\n",
    "  7 \\hfill \\\\ \n",
    "\\end{gathered}  \\right){0.5^7}{0.5^3} = 0.1172.$$\n",
    "\n",
    "To calculate $P\\left( {y\\left| {{H_1}} \\right.} \\right)$ we need to be more careful. Under $H_1$, $p$ can take two values: $0.6$ and $0.7$, therefore \n",
    "\n",
    "$$\\begin{align*}P\\left( {y\\left| {{H_1}} \\right.} \\right) &= P\\left( {y\\left| {\\left\\{ {p = 0.6} \\right\\} \\cap {H_1}} \\right.} \\right)P\\left( {p = 0.6\\left| {{H_1}} \\right.} \\right) + P\\left( {y\\left| {\\left\\{ {p = 0.7} \\right\\} \\cap {H_1}} \\right.} \\right)P\\left( {p = 0.7\\left| {{H_1}} \\right.} \\right)\\\\\n",
    "&= P\\left( {y\\left| {p = 0.6} \\right.} \\right)P\\left( {p = 0.6\\left| {{H_1}} \\right.} \\right) + P\\left( {y\\left| {p = 0.7} \\right.} \\right)P\\left( {p = 0.7\\left| {{H_1}} \\right.} \\right)\\end{align*}.$$\n",
    "\n",
    "Let's calculate $P\\left( {p = 0.6\\left| {{H_1}} \\right.} \\right)$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "  P\\left( {p = 0.6|{H_1}} \\right) &= P\\left( {p = 0.6|\\left\\{ {p = 0.6} \\right\\} \\cup \\left\\{ {p = 0.7} \\right\\}} \\right) = \\frac{{P\\left( {\\left\\{ {p = 0.6} \\right\\} \\cap \\left( {\\left\\{ {p = 0.6} \\right\\} \\cup \\left\\{ {p = 0.7} \\right\\}} \\right)} \\right)}}{{P\\left( {\\left\\{ {p = 0.6} \\right\\} \\cup \\left\\{ {p = 0.7} \\right\\}} \\right)}} \\\\ \n",
    "   &= \\frac{{P\\left( {p = 0.6} \\right)}}{{P\\left( {p = 0.6} \\right) + P\\left( {p = 0.7} \\right)}} = \\frac{{0.3}}{{0.5}} \\\\\n",
    "   &= 0.6,\n",
    "\\end{align*}$$\n",
    "\n",
    "which this implies that $P\\left( {p = 0.7|{H_1}} \\right)=0.4.$\n",
    "\n",
    "Then, \n",
    "\n",
    "$$P\\left( {y\\left| {{H_1}} \\right.} \\right) = 0.215\\times 0.6 + 0.27\\times 0.4 = 0.237.$$\n",
    "\n",
    "Now, we can plug this on the expression for the Bayes factor to obtain that\n",
    "\n",
    "$$BF = \\frac{{0.1172}}{{0.234}} = 0.49.$$\n",
    "\n",
    "Because is smaller than 1, is better to analyze the inverse $\\frac{1}{{BF}} = 2.04$, which shows that there is some evidence in favor of $H_1$.\n",
    "\n",
    "**NOTE:** When you have the posterior probabilities, the bayes factor can be calculated as \n",
    "\n",
    "$$BF = \\left( {\\frac{{P\\left( {{H_0}\\left| y \\right.} \\right)}}{{P\\left( {{H_1}\\left| y \\right.} \\right)}}} \\right)\\underbrace {\\left( {\\frac{{P\\left( {{H_1}} \\right)}}{{P\\left( {{H_0}} \\right)}}} \\right)}_1 = \\frac{{P\\left( {{H_0}\\left| y \\right.} \\right)}}{{P\\left( {{H_1}\\left| y \\right.} \\right)}} = \\frac{{0.33}}{{0.67}} = 0.49.$$\n",
    "\n",
    "Here we have used that the prior probability for both hypothesis is 0.5 (this is case in most of the problems).\n",
    "\n",
    "$$P\\left( {{H_1}} \\right) = P\\left( {\\left\\{ {p = 0.6} \\right\\} \\cup \\left\\{ {p = 0.7} \\right\\}} \\right) = \\underbrace {P\\left( {\\left\\{ {p = 0.6} \\right\\}} \\right)}_{0.3} + \\underbrace {P\\left( {\\left\\{ {p = 0.7} \\right\\}} \\right)}_{0.2} = 0.5.$$\n",
    "\n",
    "\n",
    "## Case when $H_1$ is not discrete\n",
    "\n",
    "Now, let's suppouse we want to compare $H_0: p=0.5$ against $H_1: p\\neq 0.5$. Under $H_1$ we need to choose a prior distribution for $p$, let $f(p)$ be the density of that prior. In order to calculate the Bayes factor, we just need to calculate the likelihood under $H_1$, that is $P\\left( {y\\left| {{H_1}} \\right.} \\right)$ (remeber that the likelihood under $H_0$ we already have it).\n",
    "\n",
    "The expression for $P\\left( {y\\left| {{H_1}} \\right.} \\right)$ becomes\n",
    "\n",
    "$$P\\left( {y\\left| {{H_1}} \\right.} \\right) = \\int\\limits_{ - \\infty }^{ + \\infty } {\\left( \\begin{gathered}\n",
    "  10 \\hfill \\\\\n",
    "  7 \\hfill \\\\ \n",
    "\\end{gathered}  \\right){p^7}{{\\left( {1 - p} \\right)}^3}f\\left( p \\right)dp}.$$\n",
    "\n",
    "These integrals are really complicated to calculate and most of the time we need to do numerical approximations. To calculate these integrals with PyMC3 we need to use the Sequential Monte Carlo sampler. This is a method that basically progresses by a series of successive annealed sequences from the prior to the posterior. A nice by-product of this process is that we get an estimation of the marginal likelihood. Actually for numerical reasons the returned value is the log marginal likelihood (this helps to avoid underflow).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing SMC sampler...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "Stage:   0 Beta: 1.000\n",
      "Stage:   0 Beta: 1.000\n",
      "Stage:   0 Beta: 1.000\n",
      "Stage:   0 Beta: 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1259904403340105\n"
     ]
    }
   ],
   "source": [
    "import pymc3 as pm\n",
    "import theano\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with pm.Model() as model_under_h1:\n",
    "    p = pm.Beta('p', 10, 10)\n",
    "    y = pm.Binomial('y', n=10, p=p, observed=7)\n",
    "    trace_BF_1 = pm.sample_smc(2500)\n",
    "\n",
    "# Obtaining the marginal likelihood\n",
    "likelihoods_under_h1  = np.exp(trace_BF_1.report.log_marginal_likelihood)\n",
    "# We need to take the average because the Monte-Carlo sampler\n",
    "# returns one marginal likelihood approxiamtion for each chain\n",
    "print(np.average(likelihoods_under_h1))\n",
    "\n",
    "# Storing the estimation of the likelihood under H1\n",
    "likelihood_under_h1=np.average(likelihoods_under_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bayes factor for the model is 0.9301300931191837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48190020788497656"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Under H0, the likelihood is just the probability of \n",
    "# getting 7 heads if the coin is fair\n",
    "likelihood_under_h0 = stats.binom(n=10, p=0.5).pmf(7)\n",
    "\n",
    "# Now we can calculate the Bayes factor\n",
    "bayes_factor = likelihood_under_h0/likelihood_under_h1\n",
    "print(\"The Bayes factor for the model is {0}\".format(bayes_factor))\n",
    "bayes_factor/(bayes_factor+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining posterior probabilities from Bayes factor\n",
    "\n",
    "Let \\(BF\\) be the Bayes factor. Take ${\\pi _0} = P\\left( {{H_0}} \\right)$, ${\\pi _1} = P\\left( {{H_1}} \\right)$, ${p_0} = P\\left( {{H_0}\\left| y \\right.} \\right)$ and ${p_1} = P\\left( {{H_1}\\left| y \\right.} \\right)$, then\n",
    "\n",
    "$${p_0} = \\frac{{{\\pi _0}BF}}{{{\\pi _0}BF + 1 - {\\pi _0}}}.$$\n",
    "\n",
    "This expression allows us to calculate posterior probabilities using the prior probabilities and the Bayes factor.\n",
    "\n",
    "Using this expression on the previous example, we obtain that \n",
    "\n",
    "$$P\\left( {{H_0}\\left| y \\right.} \\right) = 0.48\\quad ,\\quad P\\left( {{H_1}\\left| y \\right.} \\right) = 0.52.$$\n",
    "\n",
    "These posterior probabilities of $H_0$ are highly influenced by selection of the prior under $H_1$. The following code calculates the posterior probabilities of $H_0$ for priors of the form $Beta(\\alpha,2\\alpha)$ for different values of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing SMC sampler...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "Stage:   0 Beta: 0.419\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.449\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.440\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.446\n",
      "Stage:   1 Beta: 1.000\n",
      "Initializing SMC sampler...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "Stage:   0 Beta: 0.516\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.534\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.508\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.508\n",
      "Stage:   1 Beta: 1.000\n",
      "Initializing SMC sampler...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "Stage:   0 Beta: 0.579\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.562\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.588\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.570\n",
      "Stage:   1 Beta: 1.000\n",
      "Initializing SMC sampler...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "Stage:   0 Beta: 0.619\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.625\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.612\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.612\n",
      "Stage:   1 Beta: 1.000\n",
      "Initializing SMC sampler...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "Stage:   0 Beta: 0.681\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.649\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.675\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.665\n",
      "Stage:   1 Beta: 1.000\n",
      "Initializing SMC sampler...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "Stage:   0 Beta: 0.692\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.691\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.691\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.700\n",
      "Stage:   1 Beta: 1.000\n",
      "Initializing SMC sampler...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "Stage:   0 Beta: 0.714\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.721\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.739\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.734\n",
      "Stage:   1 Beta: 1.000\n",
      "Initializing SMC sampler...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "Stage:   0 Beta: 0.752\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.750\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.755\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.754\n",
      "Stage:   1 Beta: 1.000\n",
      "Initializing SMC sampler...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "Stage:   0 Beta: 0.787\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.796\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.807\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.808\n",
      "Stage:   1 Beta: 1.000\n",
      "Initializing SMC sampler...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "Stage:   0 Beta: 0.831\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.807\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.806\n",
      "Stage:   1 Beta: 1.000\n",
      "Stage:   0 Beta: 0.857\n",
      "Stage:   1 Beta: 1.000\n"
     ]
    }
   ],
   "source": [
    "bayes_factors = list()\n",
    "probabilities_of_h0 = list()\n",
    "for a in range(1,11):\n",
    "    with pm.Model():\n",
    "        p = pm.Beta('p', a, 2*a)\n",
    "        y = pm.Binomial('y', n=10, p=p, observed=7)\n",
    "        trace_BF_H1 = pm.sample_smc(2500)\n",
    "        # Obtaining the marginal likelihood\n",
    "        likelihoods_under_h1  = np.exp(trace_BF_H1.report.log_marginal_likelihood)\n",
    "        likelihood_under_h1 = np.average(likelihoods_under_h1)\n",
    "        # Calculating the Bayes factor for the specific prior\n",
    "        bayes_factor = likelihood_under_h0/likelihood_under_h1\n",
    "        bayes_factors.append(bayes_factor)\n",
    "        probabilities_of_h0.append(bayes_factor/(bayes_factor+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following graph, we can see the results: if the prior contains a strong belief that the coin is biased toward tail (the bigger the alpha the more biased we think the coin is), then, because the data suggest otherwise, the probability that the coin is fair increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([x for x in range(1,11)],probabilities_of_h0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use priors that suggest the coin is biased towards heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing SMC sampler...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "Stage:   0 Beta: 1.000\n",
      "Stage:   0 Beta: 1.000\n"
     ]
    }
   ],
   "source": [
    "bayes_factors = list()\n",
    "probabilities_of_h0 = list()\n",
    "for a in range(1,11):\n",
    "    with pm.Model():\n",
    "        p = pm.Beta('p', 2*a, a)\n",
    "        y = pm.Binomial('y', n=10, p=p, observed=7)\n",
    "        trace_BF_H1 = pm.sample_smc(2500)\n",
    "        # Obtaining the marginal likelihood\n",
    "        likelihoods_under_h1  = np.exp(trace_BF_H1.report.log_marginal_likelihood)\n",
    "        likelihood_under_h1 = np.average(likelihoods_under_h1)\n",
    "        # Calculating the Bayes factor for the specific prior\n",
    "        bayes_factor = likelihood_under_h0/likelihood_under_h1\n",
    "        bayes_factors.append(bayes_factor)\n",
    "        probabilities_of_h0.append(bayes_factor/(bayes_factor+1))\n",
    "plt.plot([x for x in range(1,11)],probabilities_of_h0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in this scenario, the posterior probability of $H_0$ decreases when the prior is more informative.\n",
    "\n",
    "Now let's suppouse we have more data, let's say we have $n=30$ and $21$ heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    p = pm.Beta('p', 10, 10)\n",
    "    y = pm.Binomial('y', n=30, p=p, observed=21)\n",
    "    trace_BF_1 = pm.sample_smc(2500)\n",
    "\n",
    "    # Obtaining the marginal likelihood\n",
    "    likelihoods_under_h1  = np.exp(trace_BF_1.report.log_marginal_likelihood)\n",
    "    # We need to take the average because the Monte-Carlo sampler\n",
    "    # returns one marginal likelihood approxiamtion for each chain\n",
    "    # Storing the estimation of the likelihood under H1\n",
    "    likelihood_under_h1=np.average(likelihoods_under_h1)\n",
    "    \n",
    "    # Under H0, the likelihood is just the probability of \n",
    "    # getting 7 heads if the coin is fair\n",
    "    likelihood_under_h0 = stats.binom(n=30, p=0.5).pmf(21)\n",
    "\n",
    "    # Now we can calculate the Bayes factor\n",
    "    bayes_factor = likelihood_under_h0/likelihood_under_h1\n",
    "    print(\"The Bayes factor for the model is {:.3f}, its inverse is {:.3f}\".format(bayes_factor, 1/bayes_factor))\n",
    "    print(\"The probability of H_0 given the data is {:.3f}\".format(bayes_factor/(bayes_factor+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, when we have more data, the effect of the prior decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body weight example\n",
    "\n",
    "In an example from Chapter 14 of Berry (1996), the author was interested in\n",
    "determining his true weight from a variable bathroom scale. We assume the\n",
    "measurements are normally distributed with mean $\\mu$ and standard deviation\n",
    "$\\sigma$. The author weighed himself ten times and obtained the measurements (in\n",
    "pounds) 182, 172, 173, 176, 176, 180, 173, 174, 179, and 175. For simplicity,\n",
    "assume that he knows the accuracy of the scale and $\\sigma=3$ pounds.\n",
    "\n",
    "If we let $\\mu$ denote the author’s true weight, suppose he is interested in\n",
    "assessing if his true weight is more than 175 pounds. He wishes to test the\n",
    "hypotheses\n",
    "\n",
    "$${H_0}:\\mu  \\leqslant 175,\\quad {H_1}:\\mu  > 175.$$\n",
    "\n",
    "Suppose the author has little prior knowledge about his true weight and so he\n",
    "assigns $\\mu$ a normal prior with mean 170 and standard deviation 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "data = [182, 172, 173, 176, 176, 180, 173, 174, 179, 175]\n",
    "\n",
    "with pm.Model():\n",
    "    # Normal prior for mu\n",
    "    mu = pm.Normal('mu', mu=170, sd=5)\n",
    "    # Likelihood\n",
    "    y = pm.Normal('y',mu=mu, sigma=3, observed=data)\n",
    "    # Obtaining the traces\n",
    "    trace_weights = pm.sample(2500, progressbar=True)\n",
    "    # Plot the sampled posterior using 175 as the reference value\n",
    "    pp = az.plot_posterior(trace_weights, ref_val=175)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows that, given the data, the probability of $H_0$, that is $P(\\mu\\leq 175|y)$, is just 19.2%, so we can conclude that it is unlikely that his weight is at most 175 pounds. \n",
    "\n",
    "If we were interested in calculating the Bayes factor, we could do it easily because we have the prior and posterior distributions for the parameter $\\mu$:\n",
    "\n",
    "$$BF = \\frac{{P\\left( {{H_0}\\left| y \\right.} \\right)}}{{P\\left( {{H_1}\\left| y \\right.} \\right)}}\\frac{{P\\left( {{H_1}} \\right)}}{{P\\left( {{H_0}} \\right)}} = \\frac{{P\\left( {\\mu  \\leqslant 175\\left| y \\right.} \\right)}}{{P\\left( {\\mu  > 175\\left| y \\right.} \\right)}}\\frac{{P\\left( {\\mu  > 175} \\right)}}{{P\\left( {\\mu  \\leqslant 175} \\right)}} = 0.045.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# P(mu<175) is calculated as stats.norm(loc=170,scale=5).cdf(175)\n",
    "# P(mu>175)=1-P(mu<175)\n",
    "bayes_factor = (0.192/0.808)*(1-stats.norm(loc=170,scale=5).cdf(175))/stats.norm(loc=170,scale=5).cdf(175)\n",
    "print(bayes_factor)\n",
    "1/bayes_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Bayes factor is very close to 0, we should analyze the inverse, obtaining 22.317, which indicates strong evidence against $H_0$.\n",
    "\n",
    "**IMPORTANT NOTE:** It is very important to remember that these rules are just conventions, simple guides at best. Results should always be put into context of our problems and should be accompanied with enough details so others could evaluate by themselves if they agree with our conclusions. The evidence necessary to make a claim is not the same in particle physics, or a court, or to evacuate a town to prevent hundreds of deaths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison\n",
    "\n",
    "Models should be designed as approximations to help us understand a particular problem,\n",
    "or a class of related problems. Models are not designed to be verbatim copies of the real\n",
    "world. Thus, all models are wrong in the same sense that maps are not the territory. Even\n",
    "when a priori, we consider every model to be wrong, not every model is equally wrong;\n",
    "some models will be better than others at describing a given problem. We'll study how to compare \n",
    "two or more models that are used to explain the same data. As we will learn, this is not a simple \n",
    "problem to solve and at the same time is a central problem in data analysis.\n",
    "\n",
    "The Bayesian approach to comparing hypotheses can be generalized to compare two or more models. By\n",
    "Bayes theorem\n",
    "\n",
    "$$P\\left( {\\theta \\left| y \\right.} \\right) = \\frac{{P\\left( {y\\left| \\theta  \\right.} \\right)P\\left( \\theta  \\right)}}{{P\\left( y \\right)}},$$\n",
    "\n",
    "We can make the dependency of the inference on a given model $M$ explicit and write:\n",
    "\n",
    "$$P\\left( {\\theta \\left| {y,{M_k}} \\right.} \\right) = \\frac{{P\\left( {y\\left| {\\theta ,{M_k}} \\right.} \\right)P\\left( {\\theta \\left| {{M_k}} \\right.} \\right)}}{{P\\left( {y\\left| {{M_k}} \\right.} \\right)}}.$$\n",
    "\n",
    "\n",
    "The term in the denominator is known as marginal likelihood. When doing inference, we do not need to compute this\n",
    "normalizing constant, so in practice, we often compute the posterior up to a constant factor.\n",
    "However, for model comparison and model averaging, the marginal likelihood is an\n",
    "important quantity. If our main objective is to choose only one model, the best one, from a\n",
    "set of models, we can just choose the one with the largest $P(y|M_k)$.\n",
    "\n",
    "Suppose we wish to compare two Bayesian models, where it is possible that the definition of the parameter $\\theta$ may differ between\n",
    "models. Then the Bayes factor in support of model $M_0$ is the ratio of the\n",
    "respective marginal likelihoods of the data for the two models.\n",
    "\n",
    "$$BF = \\frac{{P\\left( {y\\left| {{M_0}} \\right.} \\right)}}{{P\\left( {y\\left| {{M_1}} \\right.} \\right)}}.$$\n",
    "\n",
    "When $BF>1$, model 0 explains data better than model 1.\n",
    "\n",
    "If we let $y$ denote the vector of data and $\\theta$ the parameter,\n",
    "then a Bayesian model consists of a specification of the likelihood $f(y|\\theta)$\n",
    "and the prior density $g(\\theta)$.\n",
    "\n",
    "## Some ramarks about the likelihood\n",
    "\n",
    "Now, we will briefly discuss some key facts about the marginal likelihood. By carefully\n",
    "inspecting the definition of marginal likelihood, we can understand their properties and\n",
    "consequences for their practical use:\n",
    "\n",
    "$$p\\left( {y\\left| {{M_k}} \\right.} \\right) = \\int\\limits_{{\\theta _k}} {p\\left( {y\\left| {{\\theta _k},{M_k}} \\right.} \\right)p\\left( {{\\theta _k},{M_k}} \\right)d{\\theta _k}}.$$\n",
    "\n",
    "* **The good:** Models with more parameters have a larger penalization than models\n",
    "with fewer parameters. Bayes factor has a built-in Occam's Razor! The intuitive\n",
    "reason for this is that the larger the number of parameters, the more spread the\n",
    "prior with respect to the likelihood. Thus, when computing the integral in the\n",
    "preceding formula, you will get a smaller value with a more concentrated prior.\n",
    "\n",
    "* **The bad:** Computing the marginal likelihood is, generally, a hard task since the\n",
    "preceding formula is an integral of a highly variable function over a high\n",
    "dimensional parameter space. In general, this integral needs to be solved\n",
    "numerically using more or less sophisticated methods.\n",
    "\n",
    "* **The ugly:** The marginal likelihood depends sensitively on the values of the priors.\n",
    "\n",
    "\n",
    "Using the marginal likelihood to compare models is a good idea because a penalization for\n",
    "complex models is already included (thus preventing us from overfitting). At the same\n",
    "time, a change in the prior will affect the computations of the marginal likelihood. At first,\n",
    "this sounds a little bit silly—we already know that priors affect computations (otherwise,\n",
    "we could simply avoid them), but the point here is the word sensitively. We are talking\n",
    "about changes in the prior that will keep the inference of $\\theta$ more or less the same, but could\n",
    "have a big impact on the value of the marginal likelihood. \n",
    "\n",
    "## An example\n",
    "\n",
    "Let's Suppose we are analyzing a coin and we have could use two different priors, one is Beta(4,8)\n",
    "and the other is Beta(8,4). We would like to know which model explains the data better. Let $M_1$ be\n",
    "the mdoel with the prior Beta(8,4) and $M_2$ the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]\n",
    "\n",
    "with pm.Model() as model_1:\n",
    "    theta = pm.Beta('theta', 4, 8)\n",
    "    y = pm.Bernoulli('p',p=theta,observed=data)\n",
    "    traces_model_1 = pm.sample_smc(2500)\n",
    "\n",
    "with pm.Model() as model_2:\n",
    "    theta = pm.Beta('theta', 8, 4)\n",
    "    y = pm.Bernoulli('p',p=theta,observed=data)\n",
    "    traces_model_2 = pm.sample_smc(2500)\n",
    "\n",
    "# Obtaining the marginal likelihoods for both models\n",
    "likelihoods_under_model_1  = np.average(np.exp(traces_model_1.report.log_marginal_likelihood))\n",
    "likelihoods_under_model_2  = np.average(np.exp(traces_model_2.report.log_marginal_likelihood))\n",
    "\n",
    "# Now we can calculate the Bayes factor\n",
    "bayes_factor = likelihoods_under_model_1/likelihoods_under_model_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print('The Bayes factor is {:.2f}'.format(bayes_factor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Bayes factor strongly indicates that the model with the prior Beta(4,8) explains better the data than the model with prior Beta(8,4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
